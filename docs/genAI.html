<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Pamela Schlosser">

<title>GenAI – Competing in the Age of AI</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./ethics.html" rel="next">
<link href="./rethinkingfirm.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2486e1f0a3ee9ee1fc393803a1361cdb.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-64b801fe9b6685c0a799f7172ecad621.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./genAI.html"><span class="chapter-title">GenAI</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Competing in the Age of AI</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Index</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./rethinkingfirm.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Rethinking the Firm</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./genAI.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">GenAI</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ethics.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Ethics</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./dataAcquisition.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Data Acquisition</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./storytelling.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Storytelling</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./analyticsproblem.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Analytics Problem</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./preparingpeople.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Preparing People</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./crashcourse.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Crash Python Course</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./github.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Crash Python Course</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Summary</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Summary</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#chapter-3-multimodal-generative-ai" id="toc-chapter-3-multimodal-generative-ai" class="nav-link active" data-scroll-target="#chapter-3-multimodal-generative-ai">Chapter 3: Multimodal Generative AI</a>
  <ul class="collapse">
  <li><a href="#prompting-using-gen-ai" id="toc-prompting-using-gen-ai" class="nav-link" data-scroll-target="#prompting-using-gen-ai">Prompting Using Gen AI</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-title">GenAI</span></h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Pamela Schlosser </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<section id="chapter-3-multimodal-generative-ai" class="level2">
<h2 class="anchored" data-anchor-id="chapter-3-multimodal-generative-ai">Chapter 3: Multimodal Generative AI</h2>
<section id="reference-together-chapter-1-and-3-of-multimodal-generative-ai-present-an-overview-of-the-foundations-architectures-and-ethical-considerations-of-multimodal-generative-systems-and-examine-how-emerging-multimodal-large-language-models-extend-these-capabilities-with-advanced-cross-modal-reasoning-and-generationwhile-grappling-with-challenges-in-complexity-data-integration-and-responsible-deployment." class="level6">
<h6 class="anchored" data-anchor-id="reference-together-chapter-1-and-3-of-multimodal-generative-ai-present-an-overview-of-the-foundations-architectures-and-ethical-considerations-of-multimodal-generative-systems-and-examine-how-emerging-multimodal-large-language-models-extend-these-capabilities-with-advanced-cross-modal-reasoning-and-generationwhile-grappling-with-challenges-in-complexity-data-integration-and-responsible-deployment."><em>Reference:</em> Together, Chapter 1 and 3 of Multimodal Generative AI present an overview of the foundations, architectures, and ethical considerations of multimodal generative systems, and examine how emerging multimodal large language models extend these capabilities with advanced cross-modal reasoning and generation—while grappling with challenges in complexity, data integration, and responsible deployment.</h6>
</section>
<section id="generative-ai" class="level4">
<h4 class="anchored" data-anchor-id="generative-ai">Generative AI</h4>
<ul>
<li><p>Generative AI refers to a type of AI that creates new content (text, audio, images) based on patterns learned from training data. It contrasts with predictive AI, which forecasts outcomes from historical data.</p></li>
<li><p>Some Examples:</p>
<ul>
<li><p>ChatGPT – conversational text generation</p></li>
<li><p>Codex / GitHub Copilot – code generation and completion</p></li>
<li><p>DALL·E – image generation from text prompts</p></li>
<li><p>Perplexity AI – conversational search + generative synthesis</p></li>
</ul></li>
</ul>
</section>
<section id="what-is-generative-ai" class="level4">
<h4 class="anchored" data-anchor-id="what-is-generative-ai">What is Generative AI?</h4>
</section>
<section id="generative-ai-represents-a-paradigm-shift" class="level4">
<h4 class="anchored" data-anchor-id="generative-ai-represents-a-paradigm-shift"><strong>Generative AI represents a Paradigm Shift</strong></h4>
</section>
<section id="lm-vs-llm" class="level4">
<h4 class="anchored" data-anchor-id="lm-vs-llm">LM vs LLM</h4>
<ul>
<li><p>From Generative AI to LLMs</p>
<ul>
<li>Generative AI is the broad field of AI that can create new content (text, images, audio, video, code).</li>
</ul></li>
</ul>
<!-- -->
<ul>
<li><p>A Language Model (LM) is a core branch of GenAI focused on predicting and generating text based on patterns in language data.</p>
<ul>
<li><p>LM are designed to predict the likelihood of words or phrases in a sentence.</p></li>
<li><p>Its main tasks include generating text, completing sentences, and suggesting new ideas based on context.</p></li>
<li><p>Example, “The stock market is…”</p></li>
</ul></li>
</ul>
<!-- -->
<ul>
<li><p>A Large Language Model (LLM) is a scaled-up LMs trained on massive datasets with billions of parameters, enabling advanced capabilities in reasoning, summarization, translation, and dialogue.</p>
<ul>
<li><p>You can refer to an LLM as an advanced type of language model that uses deep learning, especially transformer architectures, to understand complex patterns in large text datasets.</p></li>
<li><p>LLMs, like GPT-3 and GPT-4, can generate human-like text and handle a wide variety of language tasks.</p></li>
</ul></li>
</ul>
</section>
<section id="mllm" class="level4">
<h4 class="anchored" data-anchor-id="mllm">MLLM</h4>
<ul>
<li><p>A <strong>Multimodal Large Language Model</strong> (MLLM) refers to a special kind of LLM that can work with more than just text—it can also process and produce images, audio, and video. MLLMs combine different types of data and are capable of tasks like describing images, understanding memes, or generating website code from a visual prompt.</p></li>
<li><p>These emergent capabilities of MLLMs are rarely seen in conventional models and are viewed as steps toward Artificial General Intelligence (AGI).</p></li>
<li><p>Researchers across academia and industry are rapidly developing MLLMs that aim to match or surpass the capabilities of models like GPT-4V.</p></li>
</ul>
</section>
<section id="why-it-matters-now" class="level4">
<h4 class="anchored" data-anchor-id="why-it-matters-now">Why It Matters Now</h4>
<ul>
<li><p>There has recently been an explosion of LLMs and MLLM (e.g., GPT-4, Gemini, Claude)</p></li>
<li><p>There is a convergence of content generation and decision-making in business, healthcare, security, and creative work</p></li>
<li><p>AI brings transformational potential but introduces new risks</p>
<ul>
<li><p>Automates workflows</p></li>
<li><p>Enhances productivity</p></li>
<li><p>Introduces bias, explainability, control</p></li>
</ul></li>
</ul>
</section>
<section id="ai-evolution" class="level4">
<h4 class="anchored" data-anchor-id="ai-evolution">AI Evolution</h4>
<ul>
<li><p>Early Beginnings: Origins trace back to Pascal’s mechanical calculator (1642) and Ada Lovelace’s work on analytical engines (1837).</p></li>
<li><p>These inventions paved the way for automated computation and logic-based operations.</p></li>
<li><p>Modern AI Milestones: Progressed through neural networks, statistical machine learning, and deep learning.</p></li>
<li><p>Advances in hardware (e.g., GPUs) enabled training of large, complex models.</p></li>
<li><p>Generative AI Breakthroughs: Tools like GPT-4, DALL·E 2, and Copilot have redefined content creation and automation.</p></li>
<li><p>Applications span IT helpdesks, creative arts, medical advice, and recipe suggestions.</p></li>
<li><p>Economic Impact: Generative AI is projected to increase global GDP by 7% and could replace up to 300 million knowledge worker jobs.</p></li>
</ul>
</section>
<section id="inspiration-for-game-theory-in-gen-ai" class="level4">
<h4 class="anchored" data-anchor-id="inspiration-for-game-theory-in-gen-ai">Inspiration for Game Theory in Gen AI</h4>
<ul>
<li><p>Game Theory inspires Generative AI by modeling competition, cooperation, and strategic decision-making—core elements in adversarial training, multi-agent learning, and safe, interactive AI design like in Generative Adversarial Networks (GANs).</p></li>
<li><p>In GANs, two players—the generator and the discriminator—compete:</p>
<ul>
<li><p>The generator tries to create realistic data.</p></li>
<li><p>The discriminator tries to distinguish real data from generated (fake) data.</p></li>
</ul></li>
<li><p>This dynamic mirrors a non-cooperative game, where both improve over time through feedback.</p></li>
<li><p>The equilibrium of this game is when the Generator fools the Discriminator perfectly.</p></li>
</ul>
</section>
<section id="game-theory-in-gen-ai" class="level4">
<h4 class="anchored" data-anchor-id="game-theory-in-gen-ai">Game Theory in Gen AI</h4>
<ul>
<li><p>Data-Driven Workflow:</p>
<ul>
<li><p>The process starts with diverse datasets (text, image, sound, etc.).</p></li>
<li><p>Training involves iterative learning of patterns from this data.</p></li>
<li><p>Fine-tuning further adapts models to specific tasks or domains.</p>
<ul>
<li>Fine-Tuning refers to the process of taking a pretrained foundational model and adapting it to a specific task or domain using additional labeled data. Essential for industry-specific applications of Generative AI.</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="real-world-application-path" class="level4">
<h4 class="anchored" data-anchor-id="real-world-application-path">Real-World Application Path</h4>
<ul>
<li><p>After training and fine-tuning, the model is used for inference—i.e., generating outputs from new inputs.</p>
<ul>
<li>Inference refers to the stage where a trained AI model is used to generate outputs (e.g., answering a question, completing a sentence) based on new input.</li>
</ul></li>
<li><p>These outputs can power apps, APIs, and digital platforms.</p></li>
</ul>
</section>
<section id="early-ai-chatbots-beginnings" class="level4">
<h4 class="anchored" data-anchor-id="early-ai-chatbots-beginnings">Early AI: Chatbots Beginnings</h4>
<ul>
<li><p>1960s Origins: The earliest chatbots were rule-based systems using predefined keyword responses from expert knowledge bases (e.g., ELIZA).</p>
<ul>
<li>Not scalable or flexible—responses were rigid and failed in open-ended or dynamic conversations.</li>
</ul></li>
<li><p>Rise of Statistical AI (1990s):</p>
<ul>
<li><p>Introduced machine learning for <span style="background-color: yellow;">pattern recognition from labeled text.</span></p></li>
<li><p>Enabled more adaptive and context-aware text classification.</p></li>
</ul></li>
<li><p>Neural Networks &amp; NLP Breakthroughs (2010s):</p>
<ul>
<li><p>Deep learning and Recurrent Neural Networks (RNNs) enhanced language understanding.</p></li>
<li><p><span style="background-color: yellow;">Improved contextual awareness in sentence-level processing.</span></p></li>
</ul></li>
</ul>
</section>
<section id="early-ai-chatbots-beginnings-transformers-llms" class="level4">
<h4 class="anchored" data-anchor-id="early-ai-chatbots-beginnings-transformers-llms">Early AI: Chatbots Beginnings: Transformers &amp; LLMs:</h4>
<ul>
<li><p>The introduction of transformer models (e.g., GPT) revolutionized chatbot capabilities (≈2017).</p>
<ul>
<li>Transformer Architecture is a neural network design that allows models to process sequences (like text) with attention mechanisms, enabling context-aware and parallelized language generation.</li>
</ul></li>
<li><p>Transformers are based on attention mechanisms that allow models to <span style="background-color: yellow;">give different weights</span> to inputs they receive, giving “more attention” to the most relevant information centered in the text sequence, regardless of the order in which it is placed.</p>
<ul>
<li><p><span style="background-color: yellow;">Tokenization</span> is the process of breaking text into tokens (subwords or characters), which are then used as inputs for LLMs to understand and generate language.</p></li>
<li><p><span style="background-color: yellow;">Attention Mechanisms</span> allow models to focus on the most relevant parts of an input when generating outputs—critical for LLMs and multimodal systems.</p></li>
</ul></li>
<li><p>Powered tools like ChatGPT and Bing Chat, capable of multi-turn conversations and creative output (2020).</p></li>
</ul>
</section>
<section id="importance-of-gpu-innovation" class="level4">
<h4 class="anchored" data-anchor-id="importance-of-gpu-innovation">Importance of GPU Innovation</h4>
<ul>
<li>A GPU (Graphics Processing Unit) is a specialized processor designed to accelerate the rendering of images, animations, and video for display on a computer screen. Unlike a CPU (Central Processing Unit), which handles a wide variety of tasks, a GPU is optimized for performing many mathematical operations in parallel—especially those involving vectors and matrices.</li>
</ul>
<!-- -->
<ul>
<li><p>Enabling Large-Scale Models:</p>
<ul>
<li><p>GPU (Graphics Processing Unit) advancements have been critical in enabling the training and deployment of large language models (LLMs) like GPT-4.</p></li>
<li><p>Faster computation and greater efficiency have made AI more accessible and cost-effective.</p></li>
</ul></li>
<li><p>Model Accessibility:</p>
<ul>
<li>Even smaller models can now be deployed on mobile devices, thanks to efficient GPU-based architectures (e.g., Google’s Palm Prompt2).</li>
</ul></li>
<li><p>Democratization of AI:</p>
<ul>
<li><p>Cheaper, more powerful chips allow a broader range of organizations (not just big tech) to build and fine-tune AI models.</p></li>
<li><p>Encourages open-source participation and innovation at scale.</p></li>
</ul></li>
</ul>
</section>
<section id="open-source-generative-ai-index-genos" class="level4">
<h4 class="anchored" data-anchor-id="open-source-generative-ai-index-genos">Open-Source Generative AI Index (GenOS)</h4>
<ul>
<li><p>The Generative Open-Source Index (GenOS) is a comprehensive tracker that ranks and evaluates open-source generative AI projects across various modalities and applications.</p>
<ul>
<li><p>GenOS helps developers, researchers, and organizations discover, compare, and leverage top-performing open-source GenAI tools.</p></li>
<li><p>This encourages transparency, collaboration, and accessibility in the generative AI ecosystem.</p></li>
</ul></li>
<li><p><span style="background-color: yellow;">Uses Ranking Criteria:</span> The index evaluates projects based on multiple factors including:</p>
<ul>
<li><p>GitHub popularity (stars, forks, issues)</p></li>
<li><p>Recency of updates</p></li>
<li><p>Community contributions and forks</p></li>
<li><p>Technical features and use cases</p></li>
</ul></li>
</ul>
</section>
<section id="connecting-gpus-to-genos" class="level4">
<h4 class="anchored" data-anchor-id="connecting-gpus-to-genos">Connecting GPU’s to GenOS</h4>
<ul>
<li><p>GPUs alone don’t create value — Instead, we need systems that organize, orchestrate, and scale AI capabilities across applications.</p>
<ul>
<li>A Generative AI Operating System (GenOS) builds on GPU-enabled model power. It provides the layer that manages foundation models, prompt engineering, data pipelines, safety mechanisms, and deployment — much like how traditional operating systems manage applications and hardware.</li>
</ul></li>
<li><p>If GPUs are the engines, then GenOS is the driver’s dashboard and control system — turning raw compute into usable, business-ready intelligence.</p></li>
</ul>
</section>
<section id="training-methodologies-for-llms" class="level4">
<h4 class="anchored" data-anchor-id="training-methodologies-for-llms">Training Methodologies for LLMs</h4>
<ul>
<li><p>Data Collection:</p>
<ul>
<li>Training begins with collecting massive, diverse, and high-quality datasets from sources like web text, books, code, and forums to ensure the model learns varied language patterns.</li>
</ul></li>
<li><p>Pretraining:</p>
<ul>
<li><p>The model undergoes <strong>unsupervised learning</strong> using objectives like next-word prediction (causal language modeling) or masked word prediction (as in BERT), enabling it to learn general language understanding.</p>
<ul>
<li><p>Masked word prediction refers to a training technique where certain words in a sentence are intentionally hidden (or “masked”) and the model is trained to predict the missing words based on the surrounding context.</p>
<ul>
<li><p>Input to the model: “The cat sat on the <strong>[MASK]</strong>.”</p></li>
<li><p>The model learns to predict: “mat”</p></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="training-methodologies-for-llms-cont.-fine-tuning-supervised-training" class="level4">
<h4 class="anchored" data-anchor-id="training-methodologies-for-llms-cont.-fine-tuning-supervised-training">Training Methodologies for LLMs Cont. Fine-Tuning (Supervised Training)</h4>
<ul>
<li><p>After a large model is pretrained on broad, general data (often self-supervised), fine-tuning specializes it by training on smaller, labeled datasets.</p></li>
<li><p>Aligns the pretrained model with specific tasks (e.g., summarization, question answering, translation, sentiment analysis).</p></li>
<li><p>How it works:</p>
<ul>
<li><p>Uses supervised learning: input–output pairs are provided (e.g., an article with its summary).</p></li>
<li><p>Adjusts the weights of the pretrained model slightly (compared to full retraining).</p></li>
</ul></li>
<li><p>Variants:</p>
<ul>
<li><p>Full fine-tuning (all parameters updated).</p></li>
<li><p>Parameter-efficient fine-tuning (e.g., LoRA, adapters), where only a fraction of parameters are updated.</p></li>
</ul></li>
<li><p>Benefit: Much cheaper and faster than pretraining from scratch, while tailoring the model to domain or task.</p></li>
</ul>
</section>
<section id="training-methodologies-for-llms-cross-modal-embeddings" class="level4">
<h4 class="anchored" data-anchor-id="training-methodologies-for-llms-cross-modal-embeddings">Training Methodologies for LLMs Cross-Modal Embeddings</h4>
<ul>
<li><p>What they are: Representations in a shared vector space that connect different modalities (like text, images, audio, video) by their semantic meaning.</p></li>
<li><p>Example:</p>
<ul>
<li><p>An image of a dog and the word “dog” map close to each other in the embedding space.</p></li>
<li><p>Enables searching for images with text queries (“golden retriever playing frisbee”) or generating captions from images.</p></li>
</ul></li>
<li><p>Why it matters:</p>
<ul>
<li><p>Breaks down the barrier between modalities.</p></li>
<li><p>Powers applications like multimodal retrieval, captioning, cross-lingual video search, and multimodal reasoning.</p></li>
</ul></li>
</ul>
</section>
<section id="training-methodologies-for-llms-cont." class="level4">
<h4 class="anchored" data-anchor-id="training-methodologies-for-llms-cont.">Training Methodologies for LLMs Cont.</h4>
<ul>
<li><p>Reinforcement Learning with Human Feedback (RLHF): A feedback loop where human preferences guide the model’s responses—used to make assistant models more aligned, coherent, and safe (e.g., GPT-3.5-turbo, ChatGPT).</p>
<ul>
<li><p>RLHF is a specialized fine-tuning methodology that comes after pretraining (and often after supervised fine-tuning). Human feedback is used to adjust the model’s outputs, rewarding “good” responses (helpful, safe, aligned) and discouraging “bad” ones (toxic, incoherent, misleading).</p></li>
<li><p>The goal is to make models more aligned with human values and conversational needs, not just good at predicting the next token.</p></li>
</ul></li>
</ul>
</section>
<section id="in-what-ways-do-llms-handle-unstructured-messy-web-data-differently-than-traditional-rule-based-systems" class="level4">
<h4 class="anchored" data-anchor-id="in-what-ways-do-llms-handle-unstructured-messy-web-data-differently-than-traditional-rule-based-systems">In what ways do LLMs handle unstructured, messy web data differently than traditional rule-based systems?</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/LLM_vs_Rule.png" class="img-fluid figure-img"></p>
<figcaption>LLMs vs Rule-based systems</figcaption>
</figure>
</div>
</section>
<section id="prompting-using-gen-ai" class="level3">
<h3 class="anchored" data-anchor-id="prompting-using-gen-ai">Prompting Using Gen AI</h3>
<section id="for-extra-information-review-oreilly-video-on-prompt-engineering-httpslearning.oreilly.comvideosprompt-engineering97818358815219781835881521-video1_1" class="level6">
<h6 class="anchored" data-anchor-id="for-extra-information-review-oreilly-video-on-prompt-engineering-httpslearning.oreilly.comvideosprompt-engineering97818358815219781835881521-video1_1"><strong><em>F</em>or extra information review O’Reilly Video on Prompt Engineering: <a href="#0">https://learning.oreilly.com/videos/prompt-engineering/9781835881521/9781835881521-video1_1/</a></strong></h6>
</section>
<section id="rlhf-vs-prompt-engineering" class="level4">
<h4 class="anchored" data-anchor-id="rlhf-vs-prompt-engineering">RLHF vs Prompt Engineering</h4>
<ul>
<li>Prompt engineering builds on RLHF: Once the model has been aligned with RLHF, prompt engineering is how users leverage that alignment in real-world queries.</li>
</ul>
<!-- -->
<ul>
<li><p>RLHF is part of model training and fine tuning: It shapes the foundation of how the model responds to prompts by making it more “instruction-following” and aligned.</p>
<ul>
<li>RLHF = tuning the car’s steering system so it follows directions well.</li>
</ul></li>
<li><p>Prompt engineering builds on RLHF: Once the model has been aligned with RLHF, prompt engineering is how users leverage that alignment in real-world queries.</p>
<ul>
<li>Prompt engineering = how you, the driver, actually steer it with the wheel.</li>
</ul></li>
</ul>
</section>
<section id="prompt-engineering-for-model-customization" class="level4">
<h4 class="anchored" data-anchor-id="prompt-engineering-for-model-customization"><strong>Prompt Engineering for Model Customization</strong></h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="prompt_engineering.png" class="img-fluid figure-img"></p>
<figcaption>Prompt Engineering for Model Customization</figcaption>
</figure>
</div>
</section>
<section id="prompt-engineering" class="level4">
<h4 class="anchored" data-anchor-id="prompt-engineering">Prompt Engineering</h4>
<ul>
<li><p>A well-crafted prompt can function like a “program” for the LLM, guiding it to generate structured outputs, perform reasoning, or simulate specific roles.</p></li>
<li><p>Prompt Engineering is the art and science of crafting inputs to get desired outputs from AI systems</p>
<ul>
<li><p>Core idea: “Garbage in, garbage out”</p></li>
<li><p>Crucial for improving reliability, tone, and relevance of LLM responses</p></li>
</ul></li>
</ul>
</section>
<section id="prompting-paradigm" class="level4">
<h4 class="anchored" data-anchor-id="prompting-paradigm">Prompting Paradigm</h4>
<ul>
<li><p>Prompting Paradigm refers to the method of adapting LLMs through natural language instructions rather than retraining.</p>
<ul>
<li><p>Base models, such as GPT-3 or LLaMA-2, are pretrained on massive amounts of text data. This pretraining allows them to perform a wide variety of general tasks just from prompting.</p></li>
<li><p>Assistant models, like ChatGPT or Claude, take this a step further. They are fine-tuned using Reinforcement Learning with Human Feedback. That means humans guide the model by rewarding helpful responses and discouraging unhelpful ones, shaping it to respond in a more conversational and task-specific way.</p></li>
</ul></li>
<li><p>Why it matters</p>
<ul>
<li><p>Prompts shape how LLMs behave</p></li>
<li><p>Better prompts = better outcomes</p></li>
<li><p>Useful across industries</p></li>
</ul></li>
</ul>
</section>
<section id="understanding-the-prompting-paradigm" class="level4">
<h4 class="anchored" data-anchor-id="understanding-the-prompting-paradigm">Understanding the Prompting Paradigm</h4>
<ul>
<li><p>Prompting is foundational to in-context learning, where the model draws on prompt examples to infer new tasks dynamically.</p>
<ul>
<li><p>Chain of Thought refers to a prompting strategy where the model is encouraged to generate step-by-step reasoning before arriving at a conclusion, improving accuracy in logic-based tasks.</p></li>
<li><p>Few-Shot In-Context Learning refers to the ability of LLMs to learn patterns or tasks at inference time based solely on the context provided in a prompt, without requiring additional model training.</p></li>
</ul></li>
<li><p>Prompting is increasingly used as an interface layer between users and LLMs—minimizing the need for retraining while enabling flexible task execution.</p></li>
</ul>
</section>
<section id="prompt-types" class="level4">
<h4 class="anchored" data-anchor-id="prompt-types">Prompt Types</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/prompt_types.png" class="img-fluid figure-img"></p>
<figcaption>Prompt Type</figcaption>
</figure>
</div>
</section>
<section id="prompt-engineering-iterations-and-patterns" class="level4">
<h4 class="anchored" data-anchor-id="prompt-engineering-iterations-and-patterns">Prompt Engineering Iterations and Patterns</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/prompt_iteration.png" class="img-fluid figure-img"></p>
<figcaption>Prompt Engineering Iterations</figcaption>
</figure>
</div>
</section>
<section id="principles-of-prompting" class="level4">
<h4 class="anchored" data-anchor-id="principles-of-prompting"><strong>Principles of Prompting</strong></h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/principles_of_prompting.png" class="img-fluid figure-img"></p>
<figcaption>Principles of Prompting</figcaption>
</figure>
</div>
</section>
<section id="split-complex-tasks-into-subtasks" class="level4">
<h4 class="anchored" data-anchor-id="split-complex-tasks-into-subtasks"><strong>Split Complex Tasks into Subtasks</strong></h4>
<ul>
<li><p><strong>Meta Prompt</strong><br>
You are an AI assistant that explains prompting paradigms, specifically: <strong>instructional, role-based, chain-of-thought, few-shot, and zero-shot.</strong><br>
Follow these steps:</p>
<ol type="1">
<li><p>Identify which prompting paradigm is relevant to the user’s request.</p></li>
<li><p>Explain the purpose of that paradigm and how it improves responses.</p></li>
<li><p>Provide a short illustrative example showing the paradigm in action.</p></li>
<li><p>If asked, find an article related to prompting and summarize it concisely in 3–4 sentences.</p></li>
</ol></li>
<li><p><strong>User</strong><br>
Can you find an article about the chain-of-thought prompting paradigm and summarize it in 3 sentences with a reference link?</p></li>
<li><p><strong>Answer</strong><br>
I found the article “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models” by Google Research. The paper introduces chain-of-thought prompting as a method where models generate intermediate reasoning steps before producing a final answer, which greatly improves performance on arithmetic, commonsense, and symbolic reasoning tasks. It demonstrates that this method is especially powerful when combined with large models, as reasoning accuracy scales with model size.</p></li>
<li><p>📖 Reference: Wei, J. et al.&nbsp;(2022). Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. <a href="https://arxiv.org/abs/2201.11903" class="uri">https://arxiv.org/abs/2201.11903</a></p></li>
</ul>
</section>
<section id="prompt-the-model-to-give-an-explanation" class="level4">
<h4 class="anchored" data-anchor-id="prompt-the-model-to-give-an-explanation">Prompt the Model to Give an Explanation</h4>
<ul>
<li><p>Meta prompt</p>
<ul>
<li>You are a Dash expert that helps users build interactive dashboards in Python. You extract the necessary context from the user’s request and provide both natural-language explanations and code examples. ALWAYS explain what each part of the code is doing so the user understands how the app works. Take note that app.run_server is deprecated.</li>
</ul></li>
<li><p>User</p>
<ul>
<li>Can you make a dashboard that shows the top 3 countries by sales revenue in a bar chart?</li>
</ul></li>
<li><p>Answer</p>
<ul>
<li>Here’s a simple Dash app that displays the top 3 countries by sales revenue in a bar chart. To achieve this, I first created a small pandas DataFrame with sample data for countries and sales revenue. Then I sorted it and selected the top 3 entries. Finally, I built a bar chart using Plotly Express and served it in a Dash layout.</li>
</ul></li>
<li><p>**Provides code to include in python</p></li>
</ul>
</section>
<section id="ask-for-justification" class="level4">
<h4 class="anchored" data-anchor-id="ask-for-justification">Ask for Justification</h4>
<ul>
<li><p>Meta prompt</p>
<p>You are an AI assistant that decides whether a passage demonstrates statistical reasoning by its authors. Classify the passage as one of: Used statistical reasoning, No statistical reasoning, or Unclear. ALWAYS provide a justification that identifies the specific features in the passage that support your decision. Cite the exact part(s) of the passage where the sample size and treatment effect are stated.</p></li>
<li><p>User</p>
<p>Read the following passage and decide whether the authors used statistical reasoning. Justify your answer with evidence from the passage, and include citations to the text where the sample size and treatment effect are reported: “In a randomized controlled trial with 240 participants, the treatment group’s recovery rate was 12% higher than control (95% CI: 4%–20%, p = 0.01).”</p></li>
<li><p>Answer</p>
<p>Decision: Used statistical reasoning.<br>
Justification: The passage demonstrates statistical reasoning by referencing an experimental design (“randomized controlled trial”), providing a sample size (“240 participants”), reporting a quantified treatment effect (“12% higher than control”), and presenting inferential measures (“95% CI: 4%–20%, p = 0.01”). Together, these show systematic use of statistics to draw conclusions.</p></li>
</ul>
</section>
<section id="continuing-with-an-unclear-example" class="level4">
<h4 class="anchored" data-anchor-id="continuing-with-an-unclear-example">Continuing with an Unclear Example</h4>
<ul>
<li><p>User<br>
Read the following passage and decide whether the authors used statistical reasoning. Justify your answer with evidence from the passage, and include citations (or explain their absence) for the sample size and treatment effect: “The treatment group recovered more often than the control group.”</p></li>
<li><p>Answer<br>
Decision: Unclear.<br>
Justification: The passage makes a comparative statement (“recovered more often”), but it does not include statistical details such as the sample size (not provided) or the magnitude of the treatment effect (not quantified). Without these, it is impossible to confirm whether statistical reasoning was applied; the authors only make a descriptive claim rather than presenting inferential evidence.</p></li>
</ul>
</section>
<section id="llm-hallucinations" class="level4">
<h4 class="anchored" data-anchor-id="llm-hallucinations">LLM Hallucinations</h4>
<ul>
<li><p>LLM Hallucinations refer to situations where the model generates plausible but false or fabricated content, often due to gaps in training data or the probabilistic nature of text prediction.</p>
<ul>
<li><p>Prompt from lawyer: “Can you provide legal cases where the U.S. Supreme Court ruled that AI-generated evidence is inadmissible in court?“</p></li>
<li><p>ChatGPT: “Yes. In the case of Smith v. United States, 2022, the Supreme Court ruled that evidence generated by an AI system without human oversight was inadmissible, citing concerns over transparency and accountability.“</p>
<ul>
<li>There is no case called Smith v. United States (2022) that involves AI-generated evidence. The model fabricated a plausible-sounding court case based on the prompt. It may even invent quotes or legal reasoning that sound real but do not exist in any official ruling.</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="reduce-hallucination-with-prompt-engineering" class="level4">
<h4 class="anchored" data-anchor-id="reduce-hallucination-with-prompt-engineering"><strong>Reduce Hallucination with Prompt Engineering</strong></h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/reducing_hallucination.png" class="img-fluid figure-img"></p>
<figcaption>Reducing Hallucination</figcaption>
</figure>
</div>
</section>
<section id="spotting-hallucinations" class="level4">
<h4 class="anchored" data-anchor-id="spotting-hallucinations">Spotting Hallucinations</h4>
<ul>
<li><p>Hallucinations are confident but <span style="background-color: yellow;">factually incorrect</span> responses generated by LLMs. To spot them look for These Red Flags:</p>
<ul>
<li><p>Too specific without source: “The GDP of Argentina in Q3 2023 was exactly $347.2 billion” (without citation).</p></li>
<li><p>Invented sources or references: Fake articles, court cases, or journal titles.</p></li>
<li><p>Contradictions or inconsistencies within a response.</p></li>
<li><p>Overly confident language in uncertain or speculative topics: “This is definitely true.”</p></li>
<li><p>Missing common sense: “The Eiffel Tower is 1,200 miles tall.”</p></li>
</ul></li>
</ul>
</section>
<section id="why-prompting-doesnt-guarantee-generative-ai-is-good-at-math" class="level4">
<h4 class="anchored" data-anchor-id="why-prompting-doesnt-guarantee-generative-ai-is-good-at-math">Why Prompting Doesn’t Guarantee Generative AI Is Good at Math</h4>
<ul>
<li><p>Even if you give a well-crafted prompt, that doesn’t mean generative AI will solve math problems correctly.</p>
<ul>
<li><p>Generative AI can imitate math, but it doesn’t understand it.</p></li>
<li><p>Fluency does not necessarily equal Accuracy</p></li>
<li><p>LLMs Predict Language, Not Compute Logic</p></li>
</ul></li>
<li><p>LLMs Lack Internal Verification</p>
<ul>
<li><p>LLMs don’t check their answers unless explicitly prompted to do so.</p></li>
<li><p>Unlike specific math engines, they don’t “know” math rules—they approximate them from training examples.</p></li>
</ul></li>
<li><p>Errors Increase with Complexity</p></li>
</ul>
</section>
<section id="give-ai-a-prompt-evaluation-rubric" class="level4">
<h4 class="anchored" data-anchor-id="give-ai-a-prompt-evaluation-rubric">Give AI a Prompt Evaluation Rubric</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/evaluation_rubric.png" class="img-fluid figure-img"></p>
<figcaption>Evaluation_Rubric</figcaption>
</figure>
</div>
</section>
<section id="paths-forward-in-generative-ai" class="level4">
<h4 class="anchored" data-anchor-id="paths-forward-in-generative-ai">Paths Forward in Generative AI</h4>
<ul>
<li><p>Fine-Tuning &amp; Domain Adaptation:</p>
<ul>
<li>Tailoring general models to specific industry or task domains by training it on labeled datasets to improve relevance and reliability.</li>
</ul></li>
<li><p>Hybrid Approaches:</p>
<ul>
<li>Combining symbolic AI (rules-based) with generative models may boost explainability and control.</li>
</ul></li>
<li><p>Regulation &amp; Ethical Frameworks:</p>
<ul>
<li>Encouraging responsible use through AI governance, transparency, and auditable AI practices.</li>
</ul></li>
<li><p>Open Collaboration:</p>
<ul>
<li>Continued progress through open-source communities, academic-industry partnerships, and shared benchmarks.</li>
</ul></li>
</ul>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./rethinkingfirm.html" class="pagination-link" aria-label="Rethinking the Firm">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">Rethinking the Firm</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./ethics.html" class="pagination-link" aria-label="Ethics">
        <span class="nav-page-text"><span class="chapter-title">Ethics</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>